import time

import joblib
import pandas as pd
import numpy as np
import sklearn
from imblearn.under_sampling import RandomUnderSampler
from sklearn import ensemble
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report, accuracy_score, roc_curve
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn import preprocessing
from deepforest import CascadeForestClassifier

app = '/news'
t_w = 0.4
# for t_w in range(1,12,1):
#
#     t_w = t_w/10.0
print(t_w)
store_path = "E:/Emnets/5G/powerMonitor/ZTE/data" + app + "/final_features_all_feature_for_prediction_" + str(
    t_w) + ".csv"
features_final_path = "E:/Emnets/5G/powerMonitor/ZTE/data" + app + "/"
df_features = pd.read_csv(store_path)
data_feature = df_features.loc[:, :].values
col = np.shape(data_feature)[1]

t_s = 2
t_ss = 1.5 * t_s

data_features = data_feature[:, 1:col - 1]
labels = data_feature[:, col - 1]
print(np.shape(labels))
for index in range(len(labels)):
    if labels[index] > t_ss:
        labels[index] = 1
    else:
        labels[index] = 0
col = np.shape(data_feature)[1]
print(data_features)
print(np.shape(data_features))
print(labels)
print(np.shape(labels))
# input()

le = preprocessing.LabelEncoder()
print(data_features[:, -4], data_features[:, -14], data_features[:, -34])
data_features[:, -4] = le.fit_transform(data_features[:, -4])
data_features[:, -14] = le.fit_transform(data_features[:, -14])
data_features[:, -34] = le.fit_transform(data_features[:, -34])
labels = le.fit_transform(labels)
# data_features[:,-3] = tran
print(data_features[:, -4], data_features[:, -14], data_features[:, -34], labels)
# input()

ss = sklearn.preprocessing.MinMaxScaler()
data_features = ss.fit_transform(data_features)
print(np.shape(data_features))
# input()

model_RandomUnderSample = RandomUnderSampler(random_state=73)  # 建立RandomUnderSampler模型对象
x_RandomUnderSampler_resampled, y_RandomUnderSampler_resampled = model_RandomUnderSample.fit_sample(data_features,
                                                                                                    labels)  # 输入数据做欠抽样处理
x_RandomUnderSampler_resampled = pd.DataFrame(x_RandomUnderSampler_resampled)
y_RandomUnderSampler_resampled = pd.DataFrame(y_RandomUnderSampler_resampled, columns=['label'])
RandomUnderSampler_resampled = pd.concat([x_RandomUnderSampler_resampled, y_RandomUnderSampler_resampled],
                                         axis=1)  # 按列合并数据框
# if not os.path.exists(features_final_path+'final_features_balanced.csv'):
# RandomUnderSampler_resampled.to_csv(
#     features_final_path + 'final_features_all_feature_for_prediction_balanced_' + str(t_w) + '.csv')
print(np.array(x_RandomUnderSampler_resampled))
print(np.array(y_RandomUnderSampler_resampled))
x_RandomUnderSampler_resampled = np.squeeze(np.array(x_RandomUnderSampler_resampled))
y_RandomUnderSampler_resampled = np.squeeze(np.array(y_RandomUnderSampler_resampled))
# input()

X_train, X_test, y_train, y_test = train_test_split(x_RandomUnderSampler_resampled, y_RandomUnderSampler_resampled,
                                                    test_size=0.3, random_state=33)
# X_train, X_test, y_train, y_test = train_test_split(data_features, labels, test_size=0.3, random_state=33)

# # "min_samples_leaf":range(1,10,1),'max_depth':range(1,20,1), 'min_samples_split':range(2,10,1), 'max_features':range(1,46,1)
# # param_grid = {"n_estimators":range(10,201,10)}
# param_grid = {"min_samples_leaf":range(1,10,1),'max_depth':range(1,20,2), 'min_samples_split':range(2,10,1), 'max_features':range(1,79,2)}
#
# # clf = RandomForestClassifier(class_weight='balanced')
# clf = RandomForestClassifier(n_estimators=150,class_weight='balanced')
#
# grid_search=GridSearchCV(clf,param_grid,scoring='roc_auc',n_jobs=-1,verbose=1,cv=5)
# grid_search.fit(X_train,y_train)
# # print(grid_search.best_params_)
# rf_best_model = grid_search.best_estimator_
#
# means = grid_search.cv_results_['mean_test_score']
# std = grid_search.cv_results_['std_test_score']
# params = grid_search.cv_results_['params']
# for mean,std,param in zip(means,std,params):
#     print("mean : %f std : %f %r" % (mean,std,param))
# print('best_estimator :',grid_search.best_estimator_)
# print('best_params :',grid_search.best_params_)
# # best_estimator : RandomForestClassifier(class_weight='balanced', max_depth=13, max_features=10,
# #                       min_samples_split=7, n_estimators=140)

# final random forest model

final_RF_model = CascadeForestClassifier(random_state=33)
# final_RF_model = RandomForestClassifier(n_estimators= 60, max_depth=11, min_samples_leaf=3,min_samples_split=9,max_features=41
#                                         ,oob_score=True, random_state=33)
# final_RF_model = RandomForestClassifier(n_estimators= 120, max_depth=11, min_samples_leaf=1,min_samples_split=3,max_features=11
#                                         ,oob_score=True, random_state=33,class_weight='balanced')
start = time.time()
final_RF_model.fit(X_train, y_train)
joblib.dump(final_RF_model,'RF_model')
end = time.time()
# final_RF_model.save(features_final_path + 'df_' + str(t_w) + '.model')
print("time:", end - start)
y_predprob4 = final_RF_model.predict_proba(X_test)[:, 1]
print('auc : %f' % (roc_auc_score(y_test, y_predprob4)))

y_pred = final_RF_model.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print("Accuracy:", acc)
print(classification_report(y_test, y_pred))

print(pd.crosstab(y_test, y_pred, rownames=['actual'], colnames=['preds']))
fpr, tpr, thersholds = roc_curve(y_test, y_predprob4)
print(fpr)
print(tpr)
